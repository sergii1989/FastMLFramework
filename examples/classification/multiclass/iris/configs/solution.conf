project_structure {
	RAW_DATA_DIR           = raw_data
	FEATURE_GENERATION_DIR = features_generation
	FEATURE_SELECTION_DIR  = features_selection
	HYPERPARAMS_OPTIM_DIR  = hyper_parameters_optimization
	SOLUTION_DIR           = single_model_solution
	ENSEMBLE_DIR           = results_ensembling
	STACKER_SUBDIR         = stacking
	BLENDER_SUBDIR         = blending
}


raw_data_settings {
	target_column = TARGET # target column (to be predicted)
	index_column  = null   # unique index column (set index_column = null if no index column in train/test data)
	                       # [note: null=None in pyhocon]
}


# -- Pre-processing part of the FastML is currently under development and will be added shortly.
# So far, an input point for a ML pipeline is the train and test data sets which are located in FEATURE_GENERATION_DIR.
# It is expected that user takes raw train and test data sets and makes all the feature engineering and transformations
# outside of the pipeline (e.g. in jupyter notebook) and then simply saves the 2 files in FEATURE_GENERATION_DIR/
# name_feats_generation_dir, where name_feats_generation_dir will be used as input point for the pipeline
# (could be different for different models):
# >>>>> see e.g. modeling_settings.lightgbm.name_feats_generation_dir
#       or modeling_settings.xgboost.name_feats_generation_dir

preprocessing {

}


# -- Features generation part of the FastML is currently under development and will be added shortly.
# So far, an input point for a ML pipeline is the train and test data sets which are located in FEATURE_GENERATION_DIR.
# It is expected that user takes raw train and test data sets and makes all the feature engineering and transformations
# outside of the pipeline (e.g. in jupyter notebook) and then simply saves the 2 files in FEATURE_GENERATION_DIR/
# name_feats_generation_dir, where name_feats_generation_dir will be used as input point for the pipeline:
# >>>>> see e.g. modeling_settings.lightgbm.name_feats_generation_dir
#       or modeling_settings.xgboost.name_feats_generation_dir

features_generation {
	train_file    = train.csv   # name of train data set file in FEATURE_GENERATION_DIR/name_feats_generation_dir
	test_file     = test.csv    # name of test data set file in FEATURE_GENERATION_DIR/name_feats_generation_dir
}


features_selection {
	name_fs_dir = fs_001  # if run_fs=True -> name of sub-directory in FEATURE_SELECTION_DIR to store selected features

	# This feature selection method is available only for binary/multiclass classification tasks
	target_permutation {
		int_threshold    = 9    # this threshold is used to limit number of int8-type numerical features to be
		                        # interpreted as categorical (see auto_selector_of_categorical_features() in utils.py)
		num_boost_rounds = 350  # number of boosting iter. for lgbm train (actual and null-importance distributions).
								# If None -> will be eval using formula (see get_feature_importances() in
								# feature_selection.py [note: null=None in pyhocon]
		nb_target_permutation_runs = 10  # should be 100+ for normal run (10 for debug)
		
		lgbm_params {
			# These parameters are used for constructing actual and null-importance distributions 
			feats_exploration {
				objective        = binary
				boosting_type    = rf
				learning_rate    = 0.1
				num_leaves       = 127
				max_depth        = 8
				subsample        = 0.623
				colsample_bytree = 0.7
				bagging_freq     = 1
				metric           = ${modeling_settings.lightgbm.eval_metric}
				verbose          = -1
			}
			# These parameters are used in features discarding process
			feats_selection {
				objective            = binary
				boosting_type        = gbdt
				learning_rate        = 0.1
				n_estimators         = 2000
				early_stopping_round = 50
				num_leaves           = 31
				max_depth            = -1
				subsample            = 0.8
				colsample_bytree     = 0.8
				min_split_gain       = 0.00001
				reg_alpha            = 0.00001
				reg_lambda           = 0.00001
				metric               = ${modeling_settings.lightgbm.eval_metric}
				verbose              = -1
				seed                 = ${modeling_settings.fs_seed_value}
			}
		}
		eval_feats_removal_impact_on_cv_score {
			thresholds   = [0, 50, 75, 90, 95, 99]  # if thresholds=[], then n_thresholds (see below) should be set.
			                                        # Then thresholds will be computed automatically
			n_thresholds = 5  #  n_thresholds is used to compute thresholds values if thresholds=[].
			                  #  If thresholds != [], n_thresholds is not used
		}
	}
}


hp_optimization {
	name_hpo_dir = hpo_001  # if run_hpo=True -> name of subdirectory in HYPERPARAMS_OPTIM_DIR to store hpo results
	
	bayes {		
		init_points = 30  # 30 number of initial points in Bayes Optimization procedure
		n_iter      = 20  # 20 number of iteration in Bayes Optimization procedure
		
		hpo_space {

			single_model_solution {
			
				# LightGBM Classifier parameters
				lightgbm {
					num_leaves        = [8, 45]
					max_depth         = [3, 8]
					learning_rate     = [0.01, 0.1]
					min_split_gain    = [0.0, 0.1]
					min_child_weight  = [0.01, 50]
					subsample         = [0.4, 1.0]
					colsample_bytree  = [0.4, 1.0]
					reg_alpha         = [0.0, 2.0]
					reg_lambda        = [0.0, 2.0]
				}

				# XGBoost Classifier parameters				
				xgboost {
					max_depth         = [3, 8]
					learning_rate     = [0.01, 0.1]
					min_child_weight  = [0.01, 50]
					gamma             = [0.001, 10]
					subsample         = [0.4, 1.0]
					colsample_bytree  = [0.4, 1.0]
					reg_alpha         = [0.0, 2.0]
					reg_lambda        = [0.0, 2.0]
				}
				
				# ExtraTrees Classifier parameters
				et {
					max_depth             = [3, 8]
					n_estimators          = [100, 1000]
					min_samples_split     = [2, 50]
					min_samples_leaf      = [1, 50]
					max_features          = [0.4, 1.0]
					max_leaf_nodes        = [2, 50]
					min_impurity_decrease = [0.0, 1.0]
				}
				
				# Logistic Regression Classifier parameters
				logistic_regression {
					tol               = [0.00001, 0.1]
					C                 = [0.001, 10]
				}
				
			}
			
			stacker_model {
			
				# LightGBM Classifier parameters
				lightgbm {
					num_leaves        = [8, 32]
					max_depth         = [3, 5]
					learning_rate     = [.01, .05]
					min_split_gain    = [0.0, 0.1]
					min_child_weight  = [0.01, 50]
					subsample         = [0.4, 1.0]
					colsample_bytree  = [0.4, 1.0]
					reg_alpha         = [0.0, 5.0]
					reg_lambda        = [0.0, 5.0]
				}
				
				# XGBoost Classifier parameters
				xgboost {
					max_depth         = [3, 5]
					learning_rate     = [.01, .05]
					min_child_weight  = [0.01, 50]
					gamma             = [0.001, 10]
					subsample         = [0.4, 1.0]
					colsample_bytree  = [0.4, 1.0]
					reg_alpha         = [0.0, 5.0]
					reg_lambda        = [0.0, 5.0]
				}
				
				# ExtraTrees Classifier parameters
				et {
					max_depth             = [3, 10]
					n_estimators          = [100, 800]
					min_samples_split     = [2, 50]
					min_samples_leaf      = [1, 50]
					max_features          = [0.4, 1.0]
					max_leaf_nodes        = [2, 50]
					min_impurity_decrease = [0.0, 1.0]
				}
	
				# Logistic Regression Classifier parameters
				logistic_regression {
					tol               = [0.00001, 0.1]
					C                 = [0.001, 10]
				}
				
				# Linear Regression Classifier parameters
				linear_regression {
				}
			}
		}		
	}
}

			
modeling_settings {
	models           = [lightgbm, xgboost, et, logistic_regression]  # models to be used for individual predictions:
	                                                                 # lightgbm, xgboost, et, logistic_regression, ..
	predict_test     = True  # if False -> run just CV and do not predict test
	debug            = False # if True -> only num_rows of original train and test data sets will be used in analysis
	num_rows         = 2000  # number of rows from train and test data sets to use in debug mode
	cols_to_exclude  = [TARGET]  # columns to be excluded from list of features used for model training 
								 # and results prediction. Note: target column is handled separately.
	
	# Stacking settings
	run_stacking         = True   # if True -> stack OOF predictions
	stack_bagged_results = False  # if True and run_bagging=True for selected model(s) -> stacker will use raw
	                              # OOF predictions obtained for each seed by the selected models (and not the
	                              # mean prediction over all seeds per model)

	# Seeds for feature selection, hyper-parameters optimization and single models prediction
	data_split_seed  = 27  # seed for train_test_split (e.g. KFold, StratifiedKFold, etc.)
	fs_seed_value    = 27  # if run_fs=True -> seed for feature selection procedure, otherwise is not used
	hpo_seed_value   = 27  # if run_hpo=True -> seed for hyper-parameter optimization procedure, otherwise is not used
	stacker_hpo_seed = 27  # if run_hpo=True -> seed for hyper-parameter optimization procedure, otherwise is not used

	# List of seeds for cv and prediction (bagging). If bagging=False, first seed from the list will be used.
	model_seeds_list = [27, 99999, 2018, 516, 986, 6846, 654456, 357951, 17971, 55599]

	# Plot settings (for classification tasks only!)
	plot_confusion_matrix   = True  # is used only for classification-type ML tasks  
	confusion_matrix_labels = null  # select labels present in the target_column that one wants to plot in the confusion
	                                # matrix. Example: confusion_matrix_labels = [0, 1, 2].
	                                # If null -> class labels are derived automatically
	labels_mapper           = null	# mapper function is used only when predict_probability=True and the target variable
	                                # is categorical. Example of mapper: lambda x: 1 if x > 0.5 else 0
									# If null -> labels_mapper will not be used in plot confusion matrix
	
	# Cross-validation and prediction settings 
	cv_params {
		metrics_scorer = accuracy_score # accuracy_score, log_loss, mean_absolute_error, etc;
		                                # see http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics
		metrics_decimals = 3     # rounding precision for metrics
		target_decimals  = 0     # rounding precision for target
		num_folds        = 3     # number of folds to be used in CV
		stratified       = True  # if set True -> preserves the percentage of samples for each class in a fold
		kfolds_shuffle   = True  # if set True -> shuffle each stratification of the data before splitting into batches
		cv_verbosity     = 50    # print info about CV training and validation errors every x iterations (e.g. 1000)
	}
	
	lightgbm {
		# Name of features dataset in a pool of features collections 
		name_feats_generation_dir = features_dataset_001
		
		# Feature selection 
		run_fs      = False               # if True -> run feature_selection procedure		
		fs_method   = target_permutation  # if run_fs=True -> name of feature selection method, otherwise is not used							
			
		# Hyper-parameters optimization
		run_hpo     = True   # if True -> run HPO of the single model	
		hpo_method  = bayes  # if run_hpo=True -> name of hpo method, otherwise is not used	
		
		# Run bagging
		run_bagging = True  # if True -> perform multiple runs of CV and prediction using all seeds from seeds_list
		
		# General settings
		predict_probability = False # if True -> use model.predict_proba(), else -> model.predict() method
		class_label         = 1     # class label(s) for which to predict the probability. Note: it is used only for
		                            # classification tasks and when predict_probability=True.
		                            # If predict_probability=False, class_label=None will be re-set automatically.
									# Class labels should be selected from the target column
								    #   - if class_label is None -> return probability of all classes in the target
								    #   - if class_label is int -> return probability of selected class
								    #   - if class_label is list of int -> return probability of selected classes  			
		eval_metric = multi_error   # evaluation metrics for lightgbm fit() method, such as auc, rmse, mae, logloss, etc.
		                            # see https://lightgbm.readthedocs.io/en/latest/Parameters.html
	}
	
	xgboost {
		# Name of features dataset in pool of features collections 
		name_feats_generation_dir = features_dataset_001
		
		# Feature selection 
		run_fs      = False               # if True -> run feature_selection procedure		
		fs_method   = target_permutation  # if run_fs=True -> name of feature selection method, otherwise is not used							
			
		# Hyper-parameters optimization
		run_hpo     = True   # if True -> run HPO of the single model	
		hpo_method  = bayes  # if run_hpo=True -> name of hpo method, otherwise is not used	
		
		# Run bagging
		run_bagging = True  # if True -> perform multiple runs of CV and prediction using all seeds from seeds_list
		
		# General settings
		predict_probability = False # if True -> use model.predict_proba(), else -> model.predict() method
		class_label         = 1     # class label(s) for which to predict the probability. Note: it is used only for
		                            # classification tasks and when predict_probability=True.
		                            # If predict_probability=False, class_label=None will be re-set automatically.
									# Class labels should be selected from the target column
								    #   - if class_label is None -> return probability of all classes in the target
								    #   - if class_label is int -> return probability of selected class
								    #   - if class_label is list of int -> return probability of selected classes
		eval_metric = merror        # evaluation metrics for xgboost fit() method, such as auc, rmse, mae, logloss, etc.
		                            # https://xgboost.readthedocs.io/en/latest/parameter.html
	}

	et {
		# Name of features dataset in pool of features collections 
		name_feats_generation_dir = features_dataset_001
		
		# Feature selection 
		run_fs      = False               # if True -> run feature_selection procedure		
		fs_method   = target_permutation  # if run_fs=True -> name of feature selection method, otherwise is not used							
			
		# Hyper-parameters optimization
		run_hpo     = True   # if True -> run HPO of the single model	
		hpo_method  = bayes  # if run_hpo=True -> name of hpo method, otherwise is not used	
		
		# Run bagging
		run_bagging = True  # if True -> perform multiple runs of CV and prediction using all seeds from seeds_list
		
		# General settings
		predict_probability = False # if True -> use model.predict_proba(), else -> model.predict() method
		class_label         = 1     # class label(s) for which to predict the probability. Note: it is used only for
		                            # classification tasks and when predict_probability=True.
		                            # If predict_probability=False, class_label=None will be re-set automatically.
									# Class labels should be selected from the target column
								    #   - if class_label is None -> return probability of all classes in the target
								    #   - if class_label is int -> return probability of selected class
								    #   - if class_label is list of int -> return probability of selected classes
		eval_metric = merror        # evaluation metrics for xgboost fit() method, such as auc, rmse, mae, logloss, etc.
		                            # http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics
	}

	logistic_regression {
		# Name of features dataset in pool of features collections 
		name_feats_generation_dir = features_dataset_001
		
		# Feature selection 
		run_fs      = False               # if True -> run feature_selection procedure		
		fs_method   = target_permutation  # if run_fs=True -> name of feature selection method, otherwise is not used							
			
		# Hyper-parameters optimization
		run_hpo     = True   # if True -> run HPO of the single model	
		hpo_method  = bayes  # if run_hpo=True -> name of hpo method, otherwise is not used	
		
		# Run bagging
		run_bagging = True  # if True -> perform multiple runs of CV and prediction using all seeds from seeds_list
		
		# General settings
		predict_probability = False # if True -> use model.predict_proba(), else -> model.predict() method
		class_label         = 1     # class label(s) for which to predict the probability. Note: it is used only for
		                            # classification tasks and when predict_probability=True.
		                            # If predict_probability=False, class_label=None will be re-set automatically.
									# Class labels should be selected from the target column
								    #   - if class_label is None -> return probability of all classes in the target
								    #   - if class_label is int -> return probability of selected class
								    #   - if class_label is list of int -> return probability of selected classes
		eval_metric = merror        # evaluation metrics for xgboost fit() method, such as auc, rmse, mae, logloss, etc.
		                            # http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics
	}
}  


single_model_init_params {

	# LightGBM Classifier parameters
	lightgbm {		
		boosting_type        = gbdt # gbdt, gbrt, rf, random_forest, dart, goss
		objective            = multiclass
		num_leaves           = 16
		max_depth            = 4 
		learning_rate        = 0.02 
		n_estimators         = 1000 
		early_stopping_round = 50
		min_split_gain       = 0.01
		min_child_weight     = 1
		subsample            = 0.8
		colsample_bytree     = 0.7
		reg_alpha            = 0.0
		reg_lambda           = 0.0
		verbose              = -1
	}
	
	# XGBoost Classifier parameters
	xgboost {		
		booster               = gbtree
		objective             = multi:softprob
		tree_method           = exact  # hist
		max_depth             = 3 
		learning_rate         = 0.05	
		n_estimators          = 500 
		early_stopping_rounds = 100
		gamma                 = 0
		min_child_weight      = 1 
		subsample             = 0.8 
		colsample_bytree      = 0.7 
		reg_alpha             = 0.0  # 0.0
		reg_lambda            = 1.0  # 1.0
		n_jobs                = -1
		verbose               = -1
		silent                = True
		# colsample_bylevel   = 0.632
		# scale_pos_weight    = 2.5
	}
	
	# ExtraTrees Classifier parameters
	et {
		n_estimators             = 100    # number of trees in the forest. 
		criterion                = gini   # quality of a split: 'gini' - Gini impurity; 'entropy' - information gain.
		max_depth                = null   # max depth of the tree (default=None). [note: null=None in pyhocon] 
		min_samples_split        = 50     # min number of samples required to split an internal node (def=2): int/float
		min_samples_leaf         = 1      # min number of samples required to be at a leaf node (def=1): int/float
		min_weight_fraction_leaf = 0.0    # min fraction of the sum total of weights req. to be at a leaf node (def=0.0)
		max_features             = 1.0    # 'auto', 'sqrt', 'log2', None -> number of feats to use when looking for best
		                                  # split (def='sqrt'). If int -> number, if float -> fraction
		max_leaf_nodes           = null   # grow trees with max_leaf_nodes in best-first fashion (int > 1 or None)
		                                  # (default=None). [note: null=None in pyhocon]
		min_impurity_decrease    = 0.0    # a node will be split if this split induces a decrease of the
		                                  # impurity >= min_impurity_decrease (default=0.0)
		bootstrap                = False  # if True -> bootstrap samples are used when building trees (default=False)
		oob_score                = False  # if True -> use oob samples to estimate the general. accuracy (default=False)
		verbose                  = 0      # controls the verbosity when fitting and predicting (default=0)
		warm_start               = False  # if True -> reuse solution of previous call to fit and add more estimators
		                                  # to the ensemble (default=False)
		# class_weight           = null   # dict, list of dicts, 'balanced', 'balanced_subsample' or None (def=None).
		                                  # [note: null=None in pyhocon]
		n_jobs                   = -1 		
	}
	
	# Logistic Regression Classifier parameters
	logistic_regression {
		penalty       = l1         # norm used in the penalization ('l1' or 'l2'), default 'l2'
		tol           = 0.00001    # tolerance for stopping criteria (default 0.0001)
		C             = 0.1        # inverse of regular. strength (def=1.0). Smaller C values -> stronger regularization
		fit_intercept = True       # constant bias to be added to the decision function, default: True
		class_weight  = balanced   # default: None. 'balanced' automat. adjust weights of classes inversely prop.
		                           # to class freq. in the input data
		solver        = liblinear  # 'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga', default: 'liblinear'
		max_iter      = 10000      # only for the newton-cg, sag and lbfgs solvers. Maximum number of iterations taken
		                           # for the solvers to converge.
		multi_class   = ovr        # 'ovr', 'multinomial', default: 'ovr' -> a binary problem is fit for each label
		verbose       = 0          # for the liblinear and lbfgs solvers set verbose to any positive num. for verbosity.
		n_jobs        = -1
	}

	# Linear Regression parameters
	linear_regression {
		fit_intercept = True   # constant bias to be added to the decision function, default: True
		normalize     = False  # default False. If True, the regressors will be normalized by subtracting the mean
		                       # and dividing by the l2-norm.
		n_jobs        = -1
	}
}


stacking_model_init_params {
	
	# LightGBM Classifier parameters
	lightgbm {
		boosting_type        = gbdt # gbdt, gbrt, rf, random_forest, dart, goss
		objective            = multiclass
		num_leaves           = 16
		max_depth            = 4 
		learning_rate        = 0.1 
		n_estimators         = 1000 
		early_stopping_round = 50
		min_split_gain       = 0.001
		min_child_weight     = 1
		subsample            = 0.7
		colsample_bytree     = 0.5
		reg_alpha            = 0.0
		reg_lambda           = 0.0
		verbose              = -1
	}
	
	# XGBoost Classifier parameters
	xgboost {
		booster               = gbtree
		objective             = multi:softprob
		grow_policy           = lossguide
		tree_method           = exact  # hist
		max_depth             = 4 
		max_delta_step        = 3  # default=0
		learning_rate         = 0.1	
		n_estimators          = 1000 
		early_stopping_rounds = 50
		gamma                 = 0
		min_child_weight      = 4 
		subsample             = 0.7 
		colsample_bytree      = 0.5 
		reg_alpha             = 0.0  # 0.0
		reg_lambda            = 1.0  # 1.0
		n_jobs                = -1
		verbose               = -1
		silent                = True
    }

	# ExtraTrees Classifier parameters
	et {
		n_estimators             = 100    # number of trees in the forest.
		criterion                = gini   # quality of a split: 'gini' - Gini impurity; 'entropy' - information gain.
		max_depth                = null   # max depth of the tree (default=None). [note: null=None in pyhocon]
		min_samples_split        = 50     # min number of samples required to split an internal node (def=2): int/float
		min_samples_leaf         = 1      # min number of samples required to be at a leaf node (def=1): int/float
		min_weight_fraction_leaf = 0.0    # min fraction of the sum total of weights req. to be at a leaf node (def=0.0)
		max_features             = 1.0    # 'auto', 'sqrt', 'log2', None -> number of feats to use when looking for best
		                                  # split (def='sqrt'). If int -> number, if float -> fraction
		max_leaf_nodes           = null   # grow trees with max_leaf_nodes in best-first fashion (int > 1 or None)
		                                  # (default=None). [note: null=None in pyhocon]
		min_impurity_decrease    = 0.0    # a node will be split if this split induces a decrease of the
		                                  # impurity >= min_impurity_decrease (default=0.0)
		bootstrap                = False  # if True -> bootstrap samples are used when building trees (default=False)
		oob_score                = False  # if True -> use oob samples to estimate the general. accuracy (default=False)
		verbose                  = 0      # controls the verbosity when fitting and predicting (default=0)
		warm_start               = False  # if True -> reuse solution of previous call to fit and add more estimators
		                                  # to the ensemble (default=False)
		# class_weight           = null   # dict, list of dicts, 'balanced', 'balanced_subsample' or None (def=None).
		                                  # [note: null=None in pyhocon]
		n_jobs                   = -1
	}

	# Logistic Regression Classifier parameters
	logistic_regression {
		penalty       = l1         # norm used in the penalization ('l1' or 'l2'), default 'l2'
		tol           = 0.00001    # tolerance for stopping criteria (default 0.0001)
		C             = 0.1        # inverse of regular. strength (def=1.0). Smaller C values -> stronger regularization
		fit_intercept = True       # constant bias to be added to the decision function, default: True
		class_weight  = balanced   # default: None. 'balanced' automat. adjust weights of classes inversely prop.
		                           # to class freq. in the input data
		solver        = liblinear  # 'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga', default: 'liblinear'
		max_iter      = 10000      # only for the newton-cg, sag and lbfgs solvers. Maximum number of iterations taken
		                           # for the solvers to converge.
		multi_class   = ovr        # 'ovr', 'multinomial', default: 'ovr' -> a binary problem is fit for each label
		verbose       = 0          # for the liblinear and lbfgs solvers set verbose to any positive num. for verbosity.
		n_jobs        = -1
	}

	# Linear Regression parameters
	linear_regression {
		fit_intercept = True   # constant bias to be added to the decision function, default: True
		normalize     = False  # default False. If True, the regressors will be normalized by subtracting the mean
		                       # and dividing by the l2-norm.
		n_jobs        = -1
	}
}

stacker {

	meta_models = [lightgbm, logistic_regression]  # lightgbm, xgboost, et, logistic_regression, linear_regression
	
	cv_params {
		num_folds        = 3     # number of folds to be used in CV
		stratified       = True  # if set True -> preserves the percentage of samples for each class in a fold
		kfolds_shuffle   = True  # if set True -> shuffle each stratification of the data before splitting into batches
		cv_verbosity     = 50    # print info about CV training and validation errors every x iterations (e.g. 1000)
	}

	# The OOF input files are used only when staking part of the pipeline is used standalone
	oof_input_files {
		lgbm_5249 {
			path = single_model_solution/lightgbm/features_dataset_001/target_permutation_fs_001/bayes_hpo_001
			files = [lgbm_bagged_OOF.csv, lgbm_bagged_SUBM.csv]
		}
		xgb_2967 {
			path = single_model_solution/xgboost/features_dataset_001/target_permutation_fs_001/bayes_hpo_001
			files = [xgb_bagged_OOF.csv, xgb_bagged_SUBM.csv]
		}
	}

	lightgbm {
		run_bagging         = False  # if True -> perform multiple runs of CV and prediction using all seeds from seeds_list
		run_hpo             = True   # if True -> run HPO of the stacking meta-model
		hpo_method          = bayes  # name of a method for stacking hpo
		use_raw_features    = False  # if True -> use raw features additionally to out-of-fold results
		predict_probability = False  # if True -> use model.predict_proba(), else -> model.predict() method
		class_label         = 1      # class label(s) for which to predict the probability. Note: it is used only for
		                             # classification tasks and when predict_probability=True.
		                             # If predict_probability=False, class_label=None will be re-set automatically.
									 # Class labels should be selected from the target column
								     #   - if class_label is None -> return probability of all classes in the target
								     #   - if class_label is int -> return probability of selected class
								     #   - if class_label is list of int -> return probability of selected classes

	    # see http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics
		metrics_scorer      = ${modeling_settings.cv_params.metrics_scorer}    # accuracy_score, log_loss, roc_auc, ..
		eval_metric         = ${modeling_settings.lightgbm.eval_metric}        # auc, rmse, mae, logloss, error, ..
		metrics_decimals    = ${modeling_settings.cv_params.metrics_decimals}  # rounding precision for metrics
		target_decimals     = ${modeling_settings.cv_params.target_decimals}   # rounding precision for target
	}
    
	xgboost {
		run_bagging         = False  # if True -> perform multiple runs of CV and pred. using all seeds from seeds_list
		run_hpo             = True   # if True -> run HPO of the stacking meta-model
		hpo_method          = bayes  # name of method for hpo
		use_raw_features    = False  # if True -> use raw features additionally to out-of-fold results
		predict_probability = False  #  if True -> use model.predict_proba(), else -> model.predict() method
		class_label         = 1      # class label(s) for which to predict the probability. Note: it is used only for
		                             # classification tasks and when predict_probability=True.
		                             # If predict_probability=False, class_label=None will be re-set automatically.
									 # Class labels should be selected from the target column
								     #   - if class_label is None -> return probability of all classes in the target
								     #   - if class_label is int -> return probability of selected class
								     #   - if class_label is list of int -> return probability of selected classes

		# see http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics
		metrics_scorer      = ${modeling_settings.cv_params.metrics_scorer}    # accuracy_score, log_loss, roc_auc, ..
		eval_metric         = ${modeling_settings.xgboost.eval_metric}         # auc, rmse, mae, logloss, error, ..
		metrics_decimals    = ${modeling_settings.cv_params.metrics_decimals}  # rounding precision for metrics
		target_decimals     = ${modeling_settings.cv_params.target_decimals}   # rounding precision for target
	}
	
	et {
		run_bagging         = False  # if True -> perform multiple runs of CV and pred. using all seeds from seeds_list
		run_hpo             = True   # if True -> run HPO of the stacking meta-model
		hpo_method          = bayes  # name of method for hpo
		use_raw_features    = False  # if True -> use raw features additionally to out-of-fold results
		predict_probability = False  # if True -> use model.predict_proba(), else -> model.predict() method
		class_label         = 1      # class label(s) for which to predict the probability. Note: it is used only for
		                             # classification tasks and when predict_probability=True.
		                             # If predict_probability=False, class_label=None will be re-set automatically.
									 # Class labels should be selected from the target column
								     #   - if class_label is None -> return probability of all classes in the target
								     #   - if class_label is int -> return probability of selected class
								     #   - if class_label is list of int -> return probability of selected classes

		# see http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics
		metrics_scorer      = ${modeling_settings.cv_params.metrics_scorer}    # accuracy_score, log_loss, roc_auc, ..
		eval_metric         = ${modeling_settings.et.eval_metric}              # auc, rmse, mae, logloss, error, ..
		metrics_decimals    = ${modeling_settings.cv_params.metrics_decimals}  # rounding precision for metrics
		target_decimals     = ${modeling_settings.cv_params.target_decimals}   # rounding precision for target
	}
	
	logistic_regression {
		run_bagging         = False  # if True -> perform multiple runs of CV and pred. using all seeds from seeds_list
		run_hpo             = True   # if True -> run HPO of the stacking meta-model
		hpo_method          = bayes  # name of method for hpo
		use_raw_features    = False  # if True -> use raw features additionally to out-of-fold results
		predict_probability = False  # if True -> use model.predict_proba(), else -> model.predict() method
		class_label         = 1      # class label(s) for which to predict the probability. Note: it is used only for
		                             # classification tasks and when predict_probability=True.
		                             # If predict_probability=False, class_label=None will be re-set automatically.
									 # Class labels should be selected from the target column
								     #   - if class_label is None -> return probability of all classes in the target
								     #   - if class_label is int -> return probability of selected class
								     #   - if class_label is list of int -> return probability of selected classes

		# see http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics
		metrics_scorer      = ${modeling_settings.cv_params.metrics_scorer}         # accuracy_score, log_loss, roc_auc, ..
		eval_metric         = ${modeling_settings.logistic_regression.eval_metric}  # auc, rmse, mae, logloss, error, ..
		metrics_decimals    = ${modeling_settings.cv_params.metrics_decimals}       # rounding precision for metrics
		target_decimals     = ${modeling_settings.cv_params.target_decimals}        # rounding precision for target
	}
	
	linear_regression {
		run_bagging         = False  # if True -> perform multiple runs of CV and pred. using all seeds from seeds_list
		run_hpo             = False  # if True -> run HPO of the stacking meta-model
		hpo_method          = bayes  # name of method for hpo
		use_raw_features    = False  # if True -> use raw features additionally to out-of-fold results
		predict_probability = False  # if True -> use model.predict_proba(), else -> model.predict() method
		class_label         = null   # class label(s) for which to predict the probability. Note: it is used only for
		                             # classification tasks and when predict_probability=True.
		                             # If predict_probability=False, class_label=None will be re-set automatically.
									 # Class labels should be selected from the target column
								     #   - if class_label is None -> return probability of all classes in the target
								     #   - if class_label is int -> return probability of selected class
								     #   - if class_label is list of int -> return probability of selected classes

		# see http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics
		metrics_scorer      = ${modeling_settings.cv_params.metrics_scorer}    # accuracy_score, log_loss, roc_auc, ..
		eval_metric         = rmse                                             # auc, rmse, mae, logloss, error, ..
		metrics_decimals    = ${modeling_settings.cv_params.metrics_decimals}  # rounding precision for metrics
		target_decimals     = ${modeling_settings.cv_params.target_decimals}   # rounding precision for target
	}
}
